<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-72811049-1', 'auto');
  ga('send', 'pageview');

</script>
  
    

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  





  

  
    
    
      
    
  

  
    
    
  

  
    
    
      
    
  

  
    
    
  


  
  <title>Whitman Bot Redux</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Sans+Pro:300,300i,600">
  <!--<link href='https://fonts.googleapis.com/css?family=Amatic+SC:400,700' rel='stylesheet' type='text/css'>-->
  <!--<link href='https://fonts.googleapis.com/css?family=Quicksand' rel='stylesheet' type='text/css'>-->
  <link rel="stylesheet" href="/style.css">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  </head>
<body>
  <div class="container">
    <header class="masthead">
  <h1 class="masthead-title--medium">
    <a href="/">HOME</a>
  </h1>
  <nav class="masthead-nav">
    
    <a href="/about-me">About</a>
    
    <a href="/blog">Blog</a>
    
    <a href="/contact">Contact</a>
    
    <a href="/assets/cv.pdf">CV</a>
    
    <a href="/projects">Projects</a>
    
    <a href="https://twitter.com/tobylunt" target="_blank">
      <i class="fa fa-twitter"></i>
    </a>
    <hr width="50%">
  </nav>
</header>


<div class="content post">
  <h1 class="post-title">Whitman Bot Redux</h1>
  <div class="post-date">
    <p><time>15 Apr 2018</time> - Read time: <span class="reading-time" title="Estimated read time">
  
  
    10 mins
  
</span></p>
    
    
     <span>[
      
      
      <a href="/tag/whitman"><code class="highligher-rouge"><nobr>whitman</nobr></code>&nbsp;</a>
      
      ]</span>
     
  </div>
  <p><a href="/whitman-bot-redux/"><img src="/assets/img/rnn.jpg" alt="RNN" /></a>
<em>image source: canonical RNN diagram, found <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">here</a></em></p>

<p>RNNs are amazing! While it isn't a surprise that we can't recreate Walt Whitman in his entirety using only a few lines of code and a GPU, we can generate highly entertaining results with a relatively small corpus and minimal training time. I had better results using a character-based approach instead of a neural net that learns a word at a time. I won't explain the basics of what an RNN is or how it works; there are many tutorials on this subject that are quite good including the one <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">from which the above diagram was found</a>, and Andrej Karpathy's excellent and now famous <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">blog post on the subject</a>. But long story short, RNNs have a 'working memory' of sorts in that the outputs are dependent not only on the immediately preceding vector of inputs, but also a history of previous inputs (of adjustable length). So for generative text, this means that the word or character that the neural net generates takes the previous context into account.</p>

<!--more-->


<p>I ended up trying two approaches (in addition to a word-level model that didn't get very far), taking advantage of the amazing public resources shared by <a href="http://www.fast.ai">fast.ai</a> and many char-RNN contributors in the name of advancing and proliferating the many applications of deep learning. First up: the Fast.ai language model from the excellent <a href="http://course.fast.ai">deep learning part 1</a> course, which predicts the next character in a sentence and can generate text following whatever style it is trained on.</p>

<p>This language model is actually only a subset of the larger goal within the lesson, however, which is a <em>sentiment classification</em> task - determining whether a movie review is positive or negative. The idea is that by first constructing and training a <em>language model</em>, we create a rudimentary "understanding" of english that can then be used for sentiment analysis. The model below was run on an Nvidia P5000 hosted by Paperspace. The fast.ai model uses some functionality from the excellent fast.ai python package, but is really just a Pytorch model with a couple added features for ease of use.</p>

<h2>CHARACTER-LEVEL MODEL: FAST.AI</h2>

<p>First, we need to read our corpus - Whitman's complete prose works, downloaded from <a href="https://whitmanarchive.org/published/other/CompleteProse.html">The Whitman Archive</a> - into a TorchText object for processing. Note that we set some key parameters here - Backprop Through Time (<code>bptt</code>) represents the length of the sequence that will be used at a given time to predict the next output, in this case characters. When the model is trained, these (in this case 8) timesteps will be "unrolled" to calculate the gradients and update weights through backpropagation. The <code>n_fac</code> parameter is the number of embeddings for each character, while <code>bs</code> is the batch size and <code>n_hidden</code> is the size of our square hidden layer matrix.</p>

<pre><code class="python"># Prep our TEXT TorchText object, and params we will use
TEXT = data.Field(lower=True, tokenize=list)
bs=64; bptt=8; n_fac=42; n_hidden=512

# build our modeldata object with train and val data, and params
FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)

# set modeldata object; min_freq is for infrequent chars
md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)
</code></pre>

<p>Now to train the model, we import the fast.ai module...</p>

<pre><code class="python">from fastai import sgdr
</code></pre>

<p>...and then assemble the architecture using standard Pytorch syntax. Here we use our parameters defined above, but also set a few others things - including <code>dropout</code>, which helps make the model more generlized and avoids overfit.</p>

<pre><code class="python">class CharSeqStatefulLSTM(nn.Module):
    def __init__(self, vocab_size, n_fac, bs, nl):
        super().__init__()
        self.vocab_size,self.nl = vocab_size,nl
        self.e = nn.Embedding(vocab_size, n_fac)
        self.rnn = nn.LSTM(n_fac, n_hidden, nl, dropout=0.7)
        self.l_out = nn.Linear(n_hidden, vocab_size)
        self.init_hidden(bs)

    def forward(self, cs):
        bs = cs[0].size(0)
        if self.h[0].size(1) != bs: self.init_hidden(bs)
        outp,h = self.rnn(self.e(cs), self.h)
        self.h = repackage_var(h)
        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)

    def init_hidden(self, bs):
        self.h = (V(torch.zeros(self.nl, bs, n_hidden)),
                  V(torch.zeros(self.nl, bs, n_hidden)))
</code></pre>

<p>We can then initialize the model object, and use one of the great features of the fast.ai library - stochastic gradient descent with restarts, which avoids getting stuck in local minima, and learning rate annealing, which helps to refine the process of gradient descent as the training progresses over multiple epochs. Often a larger learning rate is beneficial in the early stages of training, but a smaller one can lead to better result in the later stages.</p>

<pre><code class="python"># initialize the pytorch LSTM model object
m = CharSeqStatefulLSTM(md.nt, n_fac, n_hidden, 2).cuda()

# use the fast.ai LayerOptimizer to implement Adam but with SGDR and learning rate annealing. 
# 1e-2 is learning rate; 1e-5 is weight decay
lo = LayerOptimizer(optim.Adam, m, 1e-2, 1e-5)
</code></pre>

<p>Fitting the model led to some interesting results. I had a difficult time getting the loss below about 1.6. It's possible to seed the model with some initial text, or just start with a random character. Here, I seeded with "forest":</p>

<blockquote><p>forested my friends home. it another indications in thence other, with andturg and ourversomeny, take, month itself, labors in more at british vast movement—in fultworthir official cipand as i afternoon—the privilegin in a white just average, visic persons,with varied from years and kats, uncleands—the comfortable.—eaternally, new englanding able to thelochof the idea of histrick, family-bad in fancy,disb.</p></blockquote>

<p>Another attempt with different hyperparameters was slightly better, but not too different:</p>

<blockquote><p>sailing to dominary palcation. the includinate—the long togatew i am institude verycupof and dismass'd with centre, face, showif his forestus. to see my taken up and ashow'd—he radiable. stream'd. as ergars to. lucky blood, but wadoning or stranged, and they literatless, solern-vacutions of these riser" bong they own; now.—there issued, there areistone at left. a last call,' to so, be, june co.and was view of the great debart? yesterday, number'd. chirping and about by the shadow every repulsing on all drovers in her own new form's here on the gray to sooner, enceon throughter, &amp;c. spansound.</p></blockquote>

<p>So, interesting stuff, and definitely somewhat Whitman-ey. In all honesty, this is still an absolutely astounding result. The neural net has learned some basic punctuation, many complex words, and to mimic simple clauses. But we can do better.</p>

<h2>DOCKERIZED CHAR-RNN</h2>

<p>Thanks to some great humans, there is a <a href="https://hub.docker.com/r/xoryouyou/torch-rnn/">dockerized version</a> of char-rnn that can be run super duper easily. I actually got the original char-rnn running on my local machine using Torch and Lua, but it was really slow. I wanted to easily deploy the model on the GPU server I was renting, and the dockerized version was just the ticket and trained about 10x faster. Hooray!</p>

<p>Long story short, I had pretty good success with this model after some fiddling. At the end of the day the most responsive parameter was the size of the hidden layers - set in the <code>-rnn_size</code> option. Both 512 and 1024 worked quite well for me, and still trained in under an hour on my GPU. Here are a couple of snippets:</p>

<blockquote><p>Forested landscapes. I am so lear'd
to his heart for the foundation of one who should say I should say,
and in the south was a fine past tangled sea-circumstances. Then there was no
every first friends of Greece, and its entirety of our moon, I have
abandon'd his visit to the bright prosperity of the hour,
and a perfect passion of the old men who can be consciousness, and which said he
had the distinctive summer or forever quite a delicate broad songs, the
men in the rations and animals and children, even the men before
silent sounding body, and the conventional morning and great assassination
of his dead, but a present fellow, the condition of a democratic north red in
its house of victory. I say this place and subtle and
half-supplied still) I have seen the hour at the prolection day and
main to be contradictory. The idea of the middle of the same one of
the Massachusetts of the Deducation was the middle of the principle of the side of the
rest. It was troubled by the same background of the front of the like of
interrosonian, which we were a pleasant equal about the old frontier
silence, but in his own ground, and have seen these days and art or
instance.</p>

<p>"You say is it has surely be tremendous a drink at Politics and the visible of the revolutions, the
various gods and latents, however traits, and standing from the
journals, a fine stranger to the whole death.</p>

<p>The field, the long and friends, the construction of a time as the struggle of the highest
impressions, the sense of a man were flat and rivers and womaning, and
the great experiment of its body and statuette, not only in the time, the
whole beautiful world's interesting being afterward, and on the soul, which
the touch in the broad side of the trees, between the stripe, and
the masses of the matter of the universe—the oceanic seasons of
the rest. The same distinctive prailish prouders, considerations here,
and still is in the stranger, race of view out of the moral show and
poetry of the world.</p></blockquote>

<p>This one was run on a version of the corpus for which I removed the line breaks, which I actually find less interesting. While there still is no cohesive narrative, the amount of sense this auto-generated text makes, the limited number of spelling errors, and the somewhat similar style to Whitman just blows my mind.</p>

<blockquote><p>He was a most large dramatic soul, and making delicate famous thickly song-style of which he had supply the liberated me not the strong and afternoon region down to the cat-bird prayers and religion and lines of sense, nor soothing, launch'd back to the road this flying idea of the old Mississippi river—an occasionally mysterious or leaders, and by a physical democratic eminence. To-day our day in the depths of the young man, to be going in one side, the first, the anti-democratic democracy, at any rate which is a young tone about which it come to enter and in the key and a frail large fact of record, and marching away; the idea of corn. "Hall, Mr. Emerson, Mr. Again this counterpart, an untouch'd—sometimes looks to me, in the song of the farmer yet round just now to-day, my mind's breast of the windows; and there are individuals with their weed innocent. The great transparent proof of his wife generally, and leaning to get the gulls fall in the field—where I suppose a bit of literature of his regiment. She was evisted himself from the silent, frequent glass of whom I only abore perfectly farmer'd, I know abandon'd the dlead of this paracogethe of Abraham Lincoln—furnish the President and art of his capitulation, court, yet lives and coloring the mind, but always will be realized him in the side of the highest wind, and had no man and night—said that this Saguenay had made a strange speckach. ("I wish'd as little intellect, then at some curious principles, Reverent in one or outward Wall, after the suffrage overhead, as now going to be a general humanity, and much of the best thoughts in my opinion, had been arouse to come.</p></blockquote>

</div>

  
<div id="disqus_thread"></div>
<script>
var disqus_config = function () {
this.page.url = "http://tobiaslunt.com/whitman-bot-redux/"; // <--- use canonical URL
this.page.identifier = "/whitman-bot-pt2";
};
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//tobias-lunt.disqus.com/embed.js'; // <--- use Disqus shortname

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>



<!--
<div class="social-share">
    
    
        
    
    <a class="social-share-element" id="social-twitter" href="https://twitter.com/intent/tweet?text=Whitman Bot Redux&url=https://goo.gl/uxMoZJ&via=OviliaZhang&hashtags=" target="_blank" title="xShare this post on Twitter"></a>
    <a class="social-share-element" id="social-facebook" href="https://www.facebook.com/sharer/sharer.php?t=Whitman Bot Redux&u=https://goo.gl/uxMoZJ" target="_blank" title="Share this post on Facebook"></a>
</div>
-->

<!-- CUTTING OUT SOCIAL SHARING FOOTER...bc social media is dumb.
<ul class="soc">
    <li>
      <a href="https://twitter.com/intent/tweet?text=Blog post: Whitman Bot Redux&url=https://goo.gl/uxMoZJ&via=tobylunt"
         target="_blank" class="icon-social-twitter" title="Share this post on Twitter">
        <span class="icon  icon--twitter">
          <svg viewBox="0 0 512 512">
            <path d="M419.6 168.6c-11.7 5.2-24.2 8.7-37.4 10.2 13.4-8.1 23.8-20.8 28.6-36 -12.6 7.5-26.5 12.9-41.3 15.8 -11.9-12.6-28.8-20.6-47.5-20.6 -42 0-72.9 39.2-63.4 79.9 -54.1-2.7-102.1-28.6-134.2-68 -17 29.2-8.8 67.5 20.1 86.9 -10.7-0.3-20.7-3.3-29.5-8.1 -0.7 30.2 20.9 58.4 52.2 64.6 -9.2 2.5-19.2 3.1-29.4 1.1 8.3 25.9 32.3 44.7 60.8 45.2 -27.4 21.4-61.8 31-96.4 27 28.8 18.5 63 29.2 99.8 29.2 120.8 0 189.1-102.1 185-193.6C399.9 193.1 410.9 181.7 419.6 168.6z"></path>
          </svg>
        </span>
      </a>
    </li>
    <li>
      <a href="http://www.facebook.com/sharer.php?s=100&p[url]=https://goo.gl/uxMoZJ&p[title]=Blog post: Whitman Bot Redux"
         target="_blank" class="icon-social-facebook" title="Share this post on Facebook">
        <span class="icon  icon--twitter">
          <svg viewBox="0 0 512 512">
            <path d="M211.9 197.4h-36.7v59.9h36.7V433.1h70.5V256.5h49.2l5.2-59.1h-54.4c0 0 0-22.1 0-33.7 0-13.9 2.8-19.5 16.3-19.5 10.9 0 38.2 0 38.2 0V82.9c0 0-40.2 0-48.8 0 -52.5 0-76.1 23.1-76.1 67.3C211.9 188.8 211.9 197.4 211.9 197.4z"></path>
          </svg>
        </span>
      </a>
    </li>
</ul>
-->

  </div>
</body>
</html>
